---
title: 多层感知机
tags:
  - Python
  - AI
  - 神经网络
---

# 多层感知机

### 人工神经网络

- 层数和每层的神经元或者节点数是人工神经网络的主要结构组成。初始时，权重值（内部连接）和偏置量不够好。从经验中学习，才能成为一个好的决策器（分类器）。经验即数据，帮助神经网络调节权重值和偏置量。
- 调整权重值的过程被称为学习或者训练
- 一个典型的神经网络包含了大量的人工神经元（简称单元），排列在一系列不同的层中：输入层、隐层以及输出层。
- 神经网络是连接在一起的，意味着隐层中的神经元和其前的输入层以及其后的输出层的每个单元都是全连接的。

### 单层感知机

- 一个简单的线性二元分类器。
- 结合输入以及其附带的权重值来给出用来分类的输出。
- 单层感知没有隐层。
- 逻辑斯谛回归就是一个单层感知机

### 多层感知机（MLP）

多层感知机是一个反馈人工神经网络的简单例子。一个MLP除输入、输出层外。还至少有一个带节点的隐层。除输入层外，层的每个节点都被称为神经元（使用非线性激活函数，如Sigmoid或者ReLU）。MLP使用叫做反向传播的监督学习技术来训练，同时最小化损失函数，如交叉熵。使用优化器来调优参数（权重值和偏置量）。MLP的多个层以及非线性激活是区分其和线性感知机的关键。

- 多层感知机是深度神经网络的基本形式
- 线性模型：输入有多个x，记为xi，有一个输出记为y，y=ax1+bx2+……+delta(偏置)
- ![](\JODE-HRK.github.io\assets\image\线性模型示例.jpg)

### 逻辑斯谛回归模型

没有隐层只有输出层，也被称为浅神经网络或者单层神经网络。

- 下面给出一个只有一个输入时输出标签Y为0或1的二元分类问题的学习算法。给定一个输入特征向量X，求对于该输入特征X其Y=1的概率。输出层Y，即 $\sigma(Z)$，其中Z = WX+b，$\sigma$是一个Sigmoid函数

![](\JODE-HRK.github.io\assets\image\一个输入X和一个输出Y.jpg)

- 再给出一个两个输入时的二元分类问题学习算法

  给定输入特征向量X1和X2，求Y=1的概率。此时就被称为**感知机**。输出层Y，即 $\sigma(Z)$，其中Z = WX+b。

![](\JODE-HRK.github.io\assets\image\两个输入一个输出.jpg)

- 接着给出一个带有一个隐层和一个输出层的两层神经网络。两个输入特征向量X1和X2，连接着两个神经元X1' 和 X2' 。输入层到隐层的参数权重值为w1、w2、w3、w4，偏置量为b1、b2。

![](\JODE-HRK.github.io\assets\image\两层神经网络.jpg)

X1' 和 X2' 计算线性组合

![](\JODE-HRK.github.io\assets\image\两层神经网络的计算线性组合.jpg)

`(2 * 1)(2 * 2)(2 * 1)(2 * 1)`是输入层和隐层的维度

线性输入通过X1’ 和 X2‘ 通过激活单元传递到隐层中的$\alpha1$和$\alpha2$。

![](\JODE-HRK.github.io\assets\image\神经网络中的计算.jpg)

其中$\alpha1$为$\sigma(X1')$和$\alpha2$为$\sigma(X2')$，因此也能写成如下形式

![](\JODE-HRK.github.io\assets\image\如下形式.jpg)

隐层的值前向传播到输出层。输入$\alpha1$和$\alpha2$以及参数w5、w6和b3一起传播到输出层$\alpha'$

![](\JODE-HRK.github.io\assets\image\前向传播.jpg)

$\alpha' = \left[ \matrix{  w5 & w6} \right] \left[ \matrix{  a1\\ a6} \right] + [b3]$  给出了`(w5a1+w6a2)+b3`的线性组合，它将通过一个非线性Sigmoi函数传递到最终层Y，即$y = \sigma(a')$。

***

​	假设初始的一维模型结构为 `Y = wX + b`，其中参数w和b为权重值和偏置量

​	考虑损失函数，对于 w = 1, b =1 有 `L(w, b) = 0.9`。会得出`y = 1X + 1&L(w, b) = 0.9`

​	目标通过调参w和b来最小化损失函数。误差将会从输出层反向传播到隐层，再到输入层，通过调整学习率和优化器来调整参数值。最终，构建模型（回归器），即可以用X来解释Y。

​	为了开始构建模型，我们初始化权重值和偏置量。为了方便，取w=1， b = 1 作为初始值，优化器随机梯度下降取其学习率$\alpha = 0.01$。

第一步：初始设定为w=1，b=1，即Y = 1X+1	运行之后，参数被调整为w = 1.20，b=0.35

第二步：Y = 1.20X+ 0.35，再次运行后 w = 1.24 , b = 0.35

……

在不断的迭代中，权重值和偏置值逐渐趋于稳定。

同样的，在二维情况下仍然考虑参数、权重矩阵和偏置向量。在不断的迭代后，最终趋于稳定。