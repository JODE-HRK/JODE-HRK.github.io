---
title: 逻辑回归与交叉熵
tags:
  - Python
  - AI
---

# 逻辑回归与交叉熵

一、逻辑回归

相比于线性回归预测的是一个连续的值，逻辑回归预测的是 “是” 和 “否” 的回答

逻辑回归使用Sigmoid函数  (0,1)

![](\JODE-HRK.github.io\assets\image\sigmoid1.png)

sigmoid是一个概率分布函数，给定某个输入，它将输出一个概率值

为什么会给出一个概率值？

神经网络本质上是一个(表示)映射网络，每一层一个映射，最后会映射到一个0到1之间的值



二、交叉熵

平方差惩罚的是与损失为同一数量级的情形

而对于分类问题，使用交叉熵损失函数会更为有效，交叉熵会输出一个更大的“损失”

如果使用平方差，会导致你需要训练的次数非常多，训练速度非常慢

**交叉熵**刻画的是实际输出（概率）与期望输出（概率）的距离，即交叉熵的值越小，两个概率分布就越接近。假设概率分布p为期望输出，概率分布q为实际输出，H(p,q)为交叉熵

![](\JODE-HRK.github.io\assets\image\交叉熵.png)

这个公式能够放大两个概率分布之间的损失



逻辑回归损失函数

![](\JODE-HRK.github.io\assets\image\逻辑回归损失函数.png)

可以看出交叉熵的非常的大，放大了概率分布之间的损失





需要训练数据通过我的联系方式联系我哈

逻辑回归与交叉熵实例

```python
import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd.read_csv('C:/Users/Lenovo/Desktop/JODE/TensorDlowDataBase/tensorflow入门与实战-基础部分数据集/credit-a.csv')
#判断是否产生欺诈

print(data.head())  #head默认显示前5行

print(data.iloc[:, -1].value_counts())  #value_count()方法计算每个值出现的个数
#明显是一个二元的模型

x = data.iloc[:, :-1]
y = data.iloc[:, -1].replace(-1,0)  #这里使用replace方法将-1替换成0

model = tf.keras.Sequential()

model.add(tf.keras.layers.Dense(4, input_shape=(15,), activation='relu'))  #输入的数据是前15列数据

model.add(tf.keras.layers.Dense(4, activation='relu'))  #添加一个隐藏层，此时不需要在设定输入的shape，会自己判断

model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  #输出层就只有一维  逻辑回归一定在最后一层使用sigmoid函数

print(model.summary())

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['acc'] #metrics 每一次完成一个epoch，进行计算 acc == 正确率， 还可以计算损失情况
)

#进行训练
history = model.fit(x, y, epochs=100)

#可以得出，这里记录里loss（损失）的变换和acc准确度的变化
history.history.keys()

#将损失和epoch取出
plt.plot(history.epoch, history.history.get('loss'))

#此为loss的随着loss的变化
plt.show()

#正确率的表示情况
plt.plot(history.epoch, history.history.get('acc'))

#此为loss的随着loss的变化
plt.show()
```

